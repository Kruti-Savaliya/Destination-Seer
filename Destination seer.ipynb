{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efe4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "import math\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "countries = pd.read_csv(\"countries.csv\")\n",
    "test = pd.read_csv(\"test_users.csv\")\n",
    "train = pd.read_csv(\"train_users_2.csv\")\n",
    "sessions = pd.read_csv(\"sessions.csv\")\n",
    "test.head()\n",
    "train.head()\n",
    "sessions.head()\n",
    "print(test.shape)\n",
    "print(train.shape)\n",
    "print(sessions.shape)\n",
    "sessions.isnull().sum()\n",
    "sessions = sessions[sessions.user_id.notnull()]\n",
    "sessions.isnull().sum()\n",
    "sessions[sessions.action.isnull()].action_type.value_counts()\n",
    "sessions.loc[sessions.action.isnull(), 'action'] = 'message'\n",
    "sessions.isnull().sum()\n",
    "print(sessions[sessions.action_type.isnull()].action.value_counts())\n",
    "print()\n",
    "print(sessions[sessions.action_detail.isnull()].action.value_counts())\n",
    "def each_user_common_value(dataframe, feature): \n",
    "    new_df = pd.DataFrame(dataframe.groupby(['user_id','action'])[feature].value_counts())\n",
    "    new_df['index_tuple'] = new_df.index \n",
    "    new_df['count'] = new_df[feature]\n",
    "    new_columns = ['user_id','action',feature]\n",
    "    for n,col in enumerate(new_columns):\n",
    "        new_df[col] = new_df.index_tuple.apply(lambda index_tuple: index_tuple[n])\n",
    "    new_df = new_df.reset_index(drop = True)\n",
    "    new_df = new_df.drop(['index_tuple'], axis = 1)     \n",
    "    new_df_max = pd.DataFrame(new_df.groupby(['user_id','action'], as_index = False)['count'].max())\n",
    "    new_df_max = new_df_max.merge(new_df, on = ['user_id','action','count'])\n",
    "    new_df_max = new_df_max.drop('count', axis = 1)   \n",
    "    dataframe = dataframe.merge(new_df_max, left_on = ['user_id','action'], right_on = ['user_id','action'], how = 'left')\n",
    "    return dataframe\n",
    "sessions = each_user_common_value(sessions, 'action_type')\n",
    "print(\"action_type is complete.\")\n",
    "\n",
    "sessions = each_user_common_value(sessions, 'action_detail')\n",
    "print(\"action_detail is complete.\")\n",
    "sessions.loc[sessions.action_type_x.isnull(), 'action_type_x'] = sessions.action_type_y\n",
    "sessions.loc[sessions.action_detail_x.isnull(), 'action_detail_x'] = sessions.action_detail_y\n",
    "\n",
    "sessions['action_type'] = sessions.action_type_x\n",
    "sessions['action_detail'] = sessions.action_detail_x\n",
    "sessions = sessions.drop(['action_type_x','action_type_y','action_detail_x','action_detail_y'], axis = 1)\n",
    "sessions.isnull().sum()\n",
    "def all_users_common_value(dataframe, feature):\n",
    "    new_df = pd.DataFrame(dataframe.groupby('action')[feature].value_counts())\n",
    "    \n",
    "    new_df['index_tuple'] = new_df.index \n",
    "    new_df['count'] = new_df[feature]\n",
    "    \n",
    "    new_columns = ['action',feature]\n",
    "    for n,col in enumerate(new_columns):\n",
    "        new_df[col] = new_df.index_tuple.apply(lambda index_tuple: index_tuple[n])\n",
    "    \n",
    "    new_df = new_df.reset_index(drop = True)\n",
    "    new_df = new_df.drop(['index_tuple'], axis = 1) \n",
    "    \n",
    "    new_df_max = pd.DataFrame(new_df.groupby('action', as_index = False)['count'].max())\n",
    "    new_df_max = new_df_max.merge(new_df, on = ['action','count'])\n",
    "    new_df_max = new_df_max.drop('count', axis = 1)\n",
    "    \n",
    "    dataframe = dataframe.merge(new_df_max, left_on = 'action', right_on = 'action', how = 'left')\n",
    "    \n",
    "    return dataframe\n",
    "sessions = all_users_common_value(sessions, 'action_type')\n",
    "print(\"action_type is complete.\")\n",
    "sessions = all_users_common_value(sessions, 'action_detail')\n",
    "print(\"action_detail is complete.\")\n",
    "sessions.loc[sessions.action_type_x.isnull(), 'action_type_x'] = sessions.action_type_y\n",
    "sessions.loc[sessions.action_detail_x.isnull(), 'action_detail_x'] = sessions.action_detail_y\n",
    "\n",
    "sessions['action_type'] = sessions.action_type_x\n",
    "sessions['action_detail'] = sessions.action_detail_x\n",
    "sessions = sessions.drop(['action_type_x','action_type_y','action_detail_x','action_detail_y'], axis = 1)\n",
    "sessions.isnull().sum()\n",
    "sessions[sessions.action_type.isnull()].action.value_counts()\n",
    "sessions.action.value_counts()\n",
    "print(sessions[sessions.action == 'similar_listings'].action_type.value_counts())\n",
    "print(sessions[sessions.action == 'similar_listings'].action_detail.value_counts())\n",
    "sessions.loc[sessions.action == 'similar_listings_v2', 'action_type'] = \"data\"\n",
    "sessions.loc[sessions.action == 'similar_listings_v2', 'action_detail'] = \"similar_listings\"\n",
    "\n",
    "sessions.loc[sessions.action == 'lookup', 'action_type'] = \"lookup\"\n",
    "sessions.loc[sessions.action == 'lookup', 'action_detail'] = \"lookup\"\n",
    "\n",
    "sessions.loc[sessions.action == 'track_page_view', 'action_type'] = \"track_page_view\"\n",
    "sessions.loc[sessions.action == 'track_page_view', 'action_detail'] = \"track_page_view\"\n",
    "\n",
    "sessions.action_type = sessions.action_type.fillna(\"missing\")\n",
    "sessions.action_detail = sessions.action_detail.fillna(\"missing\")\n",
    "sessions.isnull().sum()\n",
    "median_duration = pd.DataFrame(sessions.groupby('action', as_index = False)['secs_elapsed'].median())\n",
    "median_duration.head()\n",
    "sessions = sessions.merge(median_duration, left_on = 'action', right_on = 'action', how = 'left')\n",
    "print(\"Merge complete.\")\n",
    "sessions.loc[sessions.secs_elapsed_x.isnull(), 'secs_elapsed_x'] = sessions.secs_elapsed_y\n",
    "print(\"Nulls are filled.\")\n",
    "sessions['secs_elapsed'] = sessions.secs_elapsed_x\n",
    "print(\"Column is created.\")\n",
    "sessions = sessions.drop(['secs_elapsed_x','secs_elapsed_y'], axis = 1)\n",
    "print(\"Columns are dropped.\")\n",
    "sessions.isnull().sum()\n",
    "sessions.head()\n",
    "sessions_summ = pd.DataFrame(sessions.user_id.value_counts(sort = False))\n",
    "sessions_summ['action_count'] = sessions_summ.user_id\n",
    "sessions_summ['user_id'] = sessions_summ.index\n",
    "sessions_summ = sessions_summ.reset_index(drop = True)\n",
    "sessions_summ.head()\n",
    "user_active = pd.DataFrame(sessions.groupby('user_id').secs_elapsed.sum())\n",
    "user_active['user_id'] = user_active.index\n",
    "user_active = user_active.reset_index(drop = True)\n",
    "sessions_summ = sessions_summ.merge(user_active)\n",
    "sessions_summ['duration'] = sessions_summ.secs_elapsed\n",
    "sessions_summ = sessions_summ.drop(\"secs_elapsed\", axis = 1)\n",
    "sessions_summ.head()\n",
    "def most_frequent_value(dataframe, feature):\n",
    "    new_df = pd.DataFrame(sessions.groupby('user_id')[feature].value_counts())\n",
    "    new_df['index_tuple'] = new_df.index\n",
    "    new_columns = ['user_id',feature]\n",
    "    for n,col in enumerate(new_columns):\n",
    "        new_df[col] = new_df.index_tuple.apply(lambda index_tuple: index_tuple[n])\n",
    "    \n",
    "    new_df = new_df.reset_index(drop = True)\n",
    "    new_df = new_df.drop('index_tuple', axis = 1)\n",
    "    new_df = new_df.groupby('user_id').first()\n",
    "    \n",
    "    new_df['user_id'] = new_df.index\n",
    "    new_df = new_df.reset_index(drop = True)\n",
    "    \n",
    "    dataframe = dataframe.merge(new_df)\n",
    "    \n",
    "    return dataframe\n",
    "sessions_f = ['action','action_type','action_detail','device_type']\n",
    "\n",
    "for f in sessions_f:\n",
    "    sessions_summ = most_frequent_value(sessions_summ, f)\n",
    "    print(\"{} is complete.\".format(f))\n",
    "sessions_summ.head()\n",
    "sessions_summ.head()\n",
    "def unique_features(feature, feature_name, dataframe):\n",
    "    unique_feature = pd.DataFrame(sessions.groupby('user_id')[feature].unique())\n",
    "    unique_feature['user_id'] = unique_feature.index\n",
    "    unique_feature = unique_feature.reset_index(drop = True)\n",
    "    unique_feature[feature_name] = unique_feature[feature].map(lambda x: len(x))\n",
    "    unique_feature = unique_feature.drop(feature, axis = 1)\n",
    "    dataframe = dataframe.merge(unique_feature, on = 'user_id')\n",
    "    return dataframe\n",
    "sessions_summ = unique_features('action', 'unique_actions', sessions_summ)\n",
    "print(\"action is complete.\")\n",
    "sessions_summ = unique_features('action_type', 'unique_action_types', sessions_summ)\n",
    "print(\"action_type is complete.\")\n",
    "sessions_summ = unique_features('action_detail', 'unique_action_details', sessions_summ)\n",
    "print(\"action_detail is complete.\")\n",
    "sessions_summ = unique_features('device_type', 'unique_device_types', sessions_summ)\n",
    "print(\"device_type is complete.\")\n",
    "sessions_summ.head()\n",
    "max_durations = pd.DataFrame(sessions.groupby(['user_id'], as_index = False)['secs_elapsed'].max())\n",
    "sessions_summ = sessions_summ.merge(max_durations, on = 'user_id')\n",
    "sessions_summ['max_time'] = sessions_summ.secs_elapsed\n",
    "sessions_summ = sessions_summ.drop('secs_elapsed', axis = 1)\n",
    "\n",
    "print(\"max_durations is complete.\")\n",
    "\n",
    "min_durations = pd.DataFrame(sessions.groupby(['user_id'], as_index = False)['secs_elapsed'].min())\n",
    "sessions_summ = sessions_summ.merge(min_durations, on = 'user_id')\n",
    "sessions_summ['min_time'] = sessions_summ.secs_elapsed\n",
    "sessions_summ = sessions_summ.drop('secs_elapsed', axis = 1)\n",
    "\n",
    "print(\"min_durations is complete.\")\n",
    "sessions_summ['avg_duration'] = sessions_summ.duration / sessions_summ.action_count\n",
    "sessions_summ.head(5)\n",
    "apple_device = ['Mac Desktop','iPhone','iPdad Tablet','iPodtouch']\n",
    "desktop_device = ['Mac Desktop','Windows Desktop','Chromebook','Linux Desktop']\n",
    "tablet_device = ['Android App Unknown Phone/Tablet','iPad Tablet','Tablet']\n",
    "mobile_device = ['Android Phone','iPhone','Windows Phone','Blackberry','Opera Phone']\n",
    "\n",
    "device_types = {'apple_device': apple_device, \n",
    "                'desktop_device': desktop_device,\n",
    "                'tablet_device': tablet_device,\n",
    "                'mobile_device': mobile_device}\n",
    "\n",
    "for device in device_types:\n",
    "    sessions_summ[device] = 0\n",
    "    sessions_summ.loc[sessions_summ.device_type.isin(device_types[device]), device] = 1\n",
    "sessions_summ.head()\n",
    "sessions_summ.isnull().sum()\n",
    "train1 = pd.merge(train,sessions_summ, left_on = train['id'], right_on = sessions_summ['user_id'], how = 'inner')\n",
    "train2 = train[~train.id.isin(train1.id)]\n",
    "train = pd.concat([train1, train2])\n",
    "\n",
    "test1 = test.merge(sessions_summ, left_on = test['id'], right_on = sessions_summ['user_id'], how = 'inner')\n",
    "test2 = test[~test.id.isin(test1.id)]\n",
    "test = pd.concat([test1, test2])\n",
    "df = pd.concat([train,test], sort =False)\n",
    "df.head()\n",
    "df.shape\n",
    "df.isnull().sum()\n",
    "df = df.drop('user_id', axis = 1)\n",
    "df = df.drop('key_0', axis = 1)\n",
    "def missing_session_data_categorical(feature):\n",
    "    return df[feature].fillna(\"missing\")\n",
    "\n",
    "def missing_session_data_continuous(feature):\n",
    "    return df[feature].fillna(0)\n",
    "session_features_cat = ['action','action_detail','action_type','device_type']\n",
    "session_features_cont = ['action_count','apple_device','desktop_device','mobile_device','tablet_device',\n",
    "                         'duration','avg_duration','max_time','min_time','unique_action_details',\n",
    "                         'unique_action_types','unique_actions','unique_device_types']\n",
    "\n",
    "for feature in session_features_cat:\n",
    "    df[feature] = missing_session_data_categorical(feature)\n",
    "    \n",
    "for feature in session_features_cont:\n",
    "    df[feature] = missing_session_data_continuous(feature)\n",
    "df.isnull().sum()\n",
    "df.action_count.describe()\n",
    "df[df.action_count > 0].action_count.describe()\n",
    "df['action_count_quartile'] = df.action_count.map(lambda x: 0 if x == 0 else (\n",
    "                                                            1 if x <= 17 else (\n",
    "                                                            2 if x <= 43 else (\n",
    "                                                            3 if x <= 97 else 4))))\n",
    "df[df.age.notnull()].age.describe()\n",
    "df.loc[df.age > 80, 'age'] = 80\n",
    "df[df.age.notnull()].age.describe()\n",
    "for feature in df.columns:\n",
    "    if(df[feature].dtype == float or df[feature].dtype == int):\n",
    "        correlation = stats.pearsonr(df[df.age.notnull()].age, df[df.age.notnull()][feature])\n",
    "        print(\"Correlation with {} = {}\".format(feature, correlation))\n",
    "df['age_group'] = df.age.map(lambda x: 0 if math.isnan(x) else (\n",
    "                                       1 if x < 18 else (\n",
    "                                       2 if x <= 33 else (\n",
    "                                       3 if x <= 42 else 4))))\n",
    "df.age = df.age.fillna(33)\n",
    "df.age.isnull().sum()\n",
    "print(df.duration.describe())\n",
    "print()\n",
    "print(df.avg_duration.describe())\n",
    "print(np.percentile(df.duration, 50))\n",
    "print(np.percentile(df.duration, 51))\n",
    "print(np.percentile(df.duration, 75))\n",
    "print()\n",
    "print(np.percentile(df.avg_duration, 50))\n",
    "print(np.percentile(df.avg_duration, 51))\n",
    "print(np.percentile(df.avg_duration, 75))\n",
    "df['duration_group'] = df.duration.map(lambda x: 0 if x == 0 else (\n",
    "                                                 1 if x <= 877166.25 else 2))\n",
    "\n",
    "df['avg_duration_group'] = df.avg_duration.map(lambda x: 0 if x == 0 else (\n",
    "                                                         1 if x <= 16553.7889474 else 2))\n",
    "print(df.duration_group.value_counts())\n",
    "print()\n",
    "print(df.avg_duration_group.value_counts())\n",
    "df.gender.value_counts()\n",
    "df.mobile_device.value_counts()\n",
    "df.signup_flow.value_counts()\n",
    "df['signup_flow_simple'] = df.signup_flow.map(lambda x: 0 if x == 0 else 1)\n",
    "df['signup_flow_simple'].value_counts()\n",
    "df.tablet_device.value_counts()\n",
    "df.date_account_created = pd.to_datetime(df.date_account_created, format='%Y-%m-%d')\n",
    "df.date_first_booking = pd.to_datetime(df.date_first_booking, format='%Y-%m-%d')\n",
    "print(df.date_account_created.min())\n",
    "print(df.date_account_created.max())\n",
    "print()\n",
    "print(df.date_first_booking.min())\n",
    "print(df.date_first_booking.max())\n",
    "calendar = USFederalHolidayCalendar()\n",
    "holidays = calendar.holidays(start = df.date_account_created.min(), \n",
    "                             end = df.date_first_booking.max())\n",
    "\n",
    "us_bd = CustomBusinessDay(calendar = USFederalHolidayCalendar())\n",
    "business_days = pd.DatetimeIndex(start = df.date_account_created.min(), \n",
    "                                 end = df.date_first_booking.max(), \n",
    "                                 freq = us_bd)\n",
    "df['year_account_created'] = df.date_account_created.dt.year\n",
    "df['month_account_created'] = df.date_account_created.dt.month\n",
    "df['weekday_account_created'] = df.date_account_created.dt.weekday\n",
    "df['business_day_account_created'] = df.date_account_created.isin(business_days)\n",
    "df['business_day_account_created'] = df.business_day_account_created.map(lambda x: 1 if x == True else 0)\n",
    "df['holiday_account_created'] = df.date_account_created.isin(holidays)\n",
    "df['holiday_account_created'] = df.holiday_account_created.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "df['year_first_booking'] = df.date_first_booking.dt.year\n",
    "df['month_first_booking'] = df.date_first_booking.dt.month\n",
    "df['weekday_first_booking'] = df.date_first_booking.dt.weekday\n",
    "df['business_day_first_booking'] = df.date_first_booking.isin(business_days)\n",
    "df['business_day_first_booking'] = df.business_day_first_booking.map(lambda x: 1 if x == True else 0)\n",
    "df['holiday_first_booking'] = df.date_first_booking.isin(holidays)\n",
    "df['holiday_first_booking'] = df.holiday_first_booking.map(lambda x: 1 if x == True else 0)\n",
    "\n",
    "df = df.drop([\"date_first_booking\",\"date_account_created\"], axis = 1)\n",
    "df.isnull().sum()\n",
    "df.year_first_booking = df.year_first_booking.fillna(min(df.year_first_booking) - 1)\n",
    "df.month_first_booking = df.month_first_booking.fillna(min(df.month_first_booking) - 1)\n",
    "df.weekday_first_booking += 1\n",
    "df.weekday_first_booking = df.weekday_first_booking.fillna(0)\n",
    "df.isnull().sum()\n",
    "df.first_affiliate_tracked.value_counts()\n",
    "df.first_affiliate_tracked = df.first_affiliate_tracked.fillna(\"untracked\")\n",
    "df.isnull().sum()\n",
    "df.head()\n",
    "df.first_browser.value_counts()\n",
    "mobile_browsers = ['Mobile Safari','Chrome Mobile','IE Mobile','Mobile Firefox','Android Browser']\n",
    "df.loc[df.first_browser.isin(mobile_browsers), \"first_browser\"] = \"Mobile\"\n",
    "cut_off = 1378\n",
    "\n",
    "other_browsers = []\n",
    "for browser, count in df.first_browser.value_counts().iteritems():\n",
    "    if count < cut_off:\n",
    "        other_browsers.append(browser)\n",
    "df.loc[df.first_browser.isin(other_browsers), \"first_browser\"] = \"Other\"\n",
    "df.first_browser.value_counts()\n",
    "df.language.value_counts()\n",
    "other_languages = []\n",
    "for language, count in df.language.value_counts().iteritems():\n",
    "    if count < 275:\n",
    "        other_languages.append(language)\n",
    "    \n",
    "print(other_languages)\n",
    "\n",
    "df.loc[df.language.isin(other_languages), \"language\"] = \"Other\"\n",
    "df.language.value_counts()\n",
    "df['not_English'] = df.language.map(lambda x: 0 if x == 'en' else 1)\n",
    "df.action.value_counts()\n",
    "other_actions = []\n",
    "for action, count in df.action.value_counts().iteritems():\n",
    "    if count < cut_off:\n",
    "        other_actions.append(action)\n",
    "    \n",
    "print(other_actions)\n",
    "\n",
    "df.loc[df.action.isin(other_actions), \"action\"] = \"Other\"\n",
    "df.action.value_counts()\n",
    "df.action_detail.value_counts()\n",
    "other_action_details = []\n",
    "for action_detail, count in df.action_detail.value_counts().iteritems():\n",
    "    if count < cut_off:\n",
    "        other_action_details.append(action_detail)\n",
    "    \n",
    "print(other_action_details)\n",
    "\n",
    "df.loc[df.action_detail.isin(other_action_details), \"action_detail\"] = \"Other\"\n",
    "df.action_detail.value_counts()\n",
    "df.action_type.value_counts()\n",
    "other_action_types = []\n",
    "for action_type, count in df.action_type.value_counts().iteritems():\n",
    "    if count < 1378:\n",
    "        other_action_types.append(action_type)\n",
    "    \n",
    "print(other_action_types)\n",
    "\n",
    "df.loc[df.action_type.isin(other_action_types), \"action_type\"] = \"Other\"\n",
    "df.action_type.value_counts()\n",
    "df.affiliate_provider.value_counts()\n",
    "other_affiliate_providers = []\n",
    "for affiliate_provider, count in df.affiliate_provider.value_counts().iteritems():\n",
    "    if count < cut_off:\n",
    "        other_affiliate_providers.append(affiliate_provider)\n",
    "    \n",
    "print(other_affiliate_providers)\n",
    "\n",
    "df.loc[df.affiliate_provider.isin(other_affiliate_providers), \"affiliate_provider\"] = \"other\"\n",
    "df.affiliate_provider.value_counts()\n",
    "df.device_type.value_counts()\n",
    "other_device_types = []\n",
    "for device_type, count in df.device_type.value_counts().iteritems():\n",
    "    if count < 1378:\n",
    "        other_device_types.append(device_type)\n",
    "    \n",
    "print(other_device_types)\n",
    "\n",
    "df.loc[df.device_type.isin(other_device_types), \"device_type\"] = \"Other\"\n",
    "df.device_type.value_counts()\n",
    "df.signup_method.value_counts()\n",
    "labels = pd.DataFrame(df.country_destination)\n",
    "df = df.drop(\"country_destination\", axis = 1)\n",
    "labels.head()\n",
    "df = df.drop('id', axis = 1)\n",
    "df.dtypes\n",
    "cont_features = []\n",
    "cat_features = []\n",
    "\n",
    "for feature in df.columns:\n",
    "    print(feature)\n",
    "    if df[feature].dtype == float or df[feature].dtype == int:\n",
    "        cont_features.append(feature)\n",
    "    elif df[feature].dtype == object:\n",
    "        cat_features.append(feature)\n",
    "print(cat_features)\n",
    "print()\n",
    "print(cont_features)\n",
    "print()\n",
    "print(len(cat_features) + len(cont_features))\n",
    "print(df.shape[1])\n",
    "date_features = ['year_account_created','month_account_created','weekday_account_created',\n",
    "                      'year_first_booking','month_first_booking','weekday_first_booking']\n",
    "for feature in date_features:\n",
    "    if feature in cont_features:\n",
    "        cont_features.remove(feature)\n",
    "    cat_features.append(feature)\n",
    "for feature in cat_features:\n",
    "    dummies = pd.get_dummies(df[feature], prefix = feature, sparse=False, drop_first = False, dtype='float')\n",
    "    df = df.drop(feature, axis = 1)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    print(\"{} is complete\".format(feature))\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "features_remain =['action_count', 'age', 'apple_device', 'avg_duration', 'desktop_device', 'duration', 'max_time',\n",
    "                  'min_time', 'mobile_device', 'signup_flow', 'tablet_device', 'timestamp_first_active',\n",
    "                  'unique_action_details', 'unique_action_types', 'unique_actions', 'unique_device_types',\n",
    "                  'action_count_quartile', 'age_group', 'duration_group', 'avg_duration_group', 'signup_flow_simple',\n",
    "                  'business_day_account_created', 'holiday_account_created','business_day_first_booking', \n",
    "                  'holiday_first_booking', 'not_English']\n",
    "for rf in features_remain:\n",
    "    df.loc[:,rf] = min_max_scaler.fit_transform(df[[rf]])\n",
    "df.dtypes\n",
    "df.dtypes\n",
    "df_train = df[:len(train)]\n",
    "df_test = df[len(train):]\n",
    "print(len(train))\n",
    "y = labels[:len(train)]\n",
    "y_dummies = pd.get_dummies(y, drop_first = False, dtype=float)\n",
    "y = pd.concat([y, y_dummies], axis=1)\n",
    "y = y.drop(\"country_destination\", axis = 1)\n",
    "y.head()\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(y.shape)\n",
    "train.country_destination.value_counts()\n",
    "y[y.columns[0]] *= len(y)/539\n",
    "y[y.columns[1]] *= len(y)/1428\n",
    "y[y.columns[2]] *= len(y)/1061\n",
    "y[y.columns[3]] *= len(y)/2249\n",
    "y[y.columns[4]] *= len(y)/5023\n",
    "y[y.columns[5]] *= len(y)/2324\n",
    "y[y.columns[6]] *= len(y)/2835\n",
    "y[y.columns[7]] *= len(y)/124543\n",
    "y[y.columns[8]] *= len(y)/762\n",
    "y[y.columns[9]] *= len(y)/217\n",
    "y[y.columns[10]] *= len(y)/62376\n",
    "y[y.columns[11]] *= len(y)/10094\n",
    "totals = []\n",
    "for i in range(12):\n",
    "    totals.append(sum(y[y.columns[i]]))\n",
    "totals\n",
    "df_train.head()\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train, y, test_size = 0.2, random_state = 2)\n",
    "x_train.head()\n",
    "inputX = x_train.as_matrix()\n",
    "inputY = y_train.as_matrix()\n",
    "inputX_test = x_test.as_matrix()\n",
    "inputY_test = y_test.as_matrix()\n",
    "input_nodes = 186\n",
    "\n",
    "mulitplier = 1.33\n",
    "\n",
    "hidden_nodes1 = 50\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "std = 1\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, input_nodes])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([input_nodes, hidden_nodes1], stddev = std))\n",
    "b1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "y1 = tf.nn.sigmoid(tf.matmul(features, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_nodes1, hidden_nodes2], stddev = std))\n",
    "b2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([hidden_nodes2, 12], stddev = std)) \n",
    "b3 = tf.Variable(tf.zeros([12]))\n",
    "y3 = tf.nn.softmax(tf.matmul(y2, W3) + b3)\n",
    "\n",
    "predictions = y3\n",
    "labels = tf.placeholder(tf.float32, [None, 12])\n",
    "training_epochs = 2000\n",
    "training_dropout = 0.6 \n",
    "display_step = 10\n",
    "n_samples = inputY.shape[1]\n",
    "batch = tf.Variable(0)\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.05,              #Base learning rate.\n",
    "  batch,             #Current index into the dataset.\n",
    "  len(inputX),       #Decay step.\n",
    "  0.95,              #Decay rate.\n",
    "  staircase=False)\n",
    "correct_prediction = tf.equal(tf.argmax(predictions,1), tf.argmax(labels,1))\n",
    "correct_top5 = tf.nn.in_top_k(predictions, tf.argmax(labels, 1), k = 5)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy_top5 = tf.reduce_mean(tf.cast(correct_top5, tf.float32))\n",
    "\n",
    "print('Accuracy function created.')\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(tf.clip_by_value(predictions,1e-10,1.0)))\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "accuracy_summary = [] \n",
    "accuracy_top5_summary = [] \n",
    "loss_summary = []\n",
    "\n",
    "test_accuracy_summary = []\n",
    "test_accuracy_top5_summary = [] \n",
    "test_loss_summary = [] \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "for i in range(training_epochs):  \n",
    "    session.run([optimizer], \n",
    "                feed_dict={features: inputX, \n",
    "                           labels: inputY,\n",
    "                           pkeep: training_dropout})\n",
    "\n",
    "    if (i) % display_step == 0:\n",
    "        train_accuracy, train_accuracy_top5, newLoss = session.run([accuracy,accuracy_top5,loss], \n",
    "                                                                   feed_dict={features: inputX, \n",
    "                                                                              labels: inputY,\n",
    "                                                                              pkeep: training_dropout})\n",
    "        print (\"Epoch:\", i,\n",
    "               \"Accuracy =\", \"{:.6f}\".format(train_accuracy), \n",
    "               \"Top 5 Accuracy =\", \"{:.6f}\".format(train_accuracy_top5),\n",
    "               \"Loss = \", \"{:.6f}\".format(newLoss))\n",
    "        accuracy_summary.append(train_accuracy)\n",
    "        accuracy_top5_summary.append(train_accuracy_top5)\n",
    "        loss_summary.append(newLoss)\n",
    "        \n",
    "        test_accuracy,test_accuracy_top5,test_newLoss = session.run([accuracy,accuracy_top5,loss], \n",
    "                                                              feed_dict={features: inputX_test, \n",
    "                                                                         labels: inputY_test,\n",
    "                                                                         pkeep: 1})\n",
    "        print (\"Epoch:\", i,\n",
    "               \"Test-Accuracy =\", \"{:.6f}\".format(test_accuracy), \n",
    "               \"Test-Top 5 Accuracy =\", \"{:.6f}\".format(test_accuracy_top5),\n",
    "               \"Test-Loss = \", \"{:.6f}\".format(test_newLoss))\n",
    "        test_accuracy_summary.append(test_accuracy)\n",
    "        test_accuracy_top5_summary.append(test_accuracy_top5)\n",
    "        test_loss_summary.append(test_newLoss)\n",
    "\n",
    "print()\n",
    "print (\"Optimization Finished!\")\n",
    "training_accuracy, training_top5_accuracy = session.run([accuracy,accuracy_top5], \n",
    "                                feed_dict={features: inputX, labels: inputY, pkeep: training_dropout})\n",
    "print (\"Training Accuracy=\", training_accuracy)\n",
    "print (\"Training Top 5 Accuracy=\", training_top5_accuracy)\n",
    "print()\n",
    "testing_accuracy, testing_top5_accuracy = session.run([accuracy,accuracy_top5], \n",
    "                                                       feed_dict={features: inputX_test, \n",
    "                                                                  labels: inputY_test,\n",
    "                                                                  pkeep: 1})\n",
    "print (\"Testing Accuracy=\", testing_accuracy)\n",
    "print (\"Testing Top 5 Accuracy=\", testing_top5_accuracy)\n",
    "testing_predictions, testing_labels = session.run([tf.argmax(predictions,1), tf.argmax(labels,1)], \n",
    "                                                  feed_dict={features: inputX_test,\n",
    "                                                             labels: inputY_test,\n",
    "                                                             pkeep: 1})\n",
    "\n",
    "print(classification_report(testing_labels, testing_predictions, target_names=y.columns))\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(10,8))\n",
    "\n",
    "ax1.plot(accuracy_summary)\n",
    "ax1.plot(test_accuracy_summary)\n",
    "ax1.set_title('Top 1 Accuracy')\n",
    "\n",
    "ax2.plot(accuracy_top5_summary)\n",
    "ax2.plot(test_accuracy_top5_summary)\n",
    "ax2.set_title('Top 5 Accuracy')\n",
    "\n",
    "ax3.plot(loss_summary)\n",
    "ax3.plot(test_loss_summary)\n",
    "ax3.set_title('Loss')\n",
    "\n",
    "plt.xlabel('Epochs (x10)')\n",
    "plt.show()\n",
    "test_final = df_test.as_matrix()\n",
    "final_probabilities = session.run(predictions, feed_dict={features: test_final,\n",
    "                                                          pkeep: 1})\n",
    "final_probabilities[0]\n",
    "le = LabelEncoder()\n",
    "fit_labels = le.fit_transform(train.country_destination) \n",
    "\n",
    "test_getIDs = pd.read_csv(\"test_users.csv\")\n",
    "testIDs = test_getIDs['id']\n",
    "\n",
    "ids = []  \n",
    "countries = []  \n",
    "for i in range(len(testIDs)):\n",
    "    idx = testIDs[i]\n",
    "    ids += [idx] * 5\n",
    "    countries += le.inverse_transform(np.argsort(final_probabilities[i])[::-1])[:5].tolist()\n",
    "    if i % 10000 == 0:\n",
    "        print (\"Percent complete: {}%\".format(round(i / len(test),4)*100))\n",
    "\n",
    "submission = pd.DataFrame(np.column_stack((ids, countries)), columns=['id', 'country'])\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "submission.head(25)\n",
    "submission.country.value_counts()\n",
    "train.country_destination.value_counts()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
